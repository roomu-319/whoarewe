<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sequence to Sequence Example</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            padding: 20px;
        }
        h1 {
            color: #4CAF50;
        }
        pre {
            background-color: #f4f4f4;
            padding: 15px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }
    </style>
</head>
<body>
<h1>Xor problem</h1>

<pre>
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
inputs=np.array([[0,0],[0,1],[1,0],[1,1]])
outputs=np.array([[0],[1],[1],[0]])
model=Sequential()
model.add(Dense(2,input_dim=2,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.fit(inputs,outputs,epochs=5000,verbose=0)
print(model.evaluate(inputs,outputs))

<a href="https://colab.research.google.com/drive/17FH2_UGuJ0jEN5MMT3c5bEtHxDfgwsIf?usp=sharing" target="_blank">
        Open Google Colab</a>
</pre>

<h1>Character recognition cnn</h1>
<pre>
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
train_images= train_images.reshape((60000, 28, 28, 1)).astype('float32')/255
test_images= test_images.reshape((10000, 28, 28, 1)).astype('float32')/255
train_labels= tf.keras.utils.to_categorical(train_labels)
test_labels= tf.keras.utils.to_categorical(test_labels)
model= models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history= model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)
test_loss, test_acc = model.evaluate(test_images, test_labels,verbose=0)
print(f'Test accuracy: {test_acc}')

<a href="https://colab.research.google.com/drive/1OMW1aOopKwgNM3y2yUUn6_UriT3sCbJ7?usp=sharing" target="_blank">
        Open Google Colab</a>
</pre>

<h1>FaceRecognition</h1>
<pre>
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
import numpy as np

lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

n_samples, h, w = lfw_people.images.shape
X = lfw_people.images
y = lfw_people.target
n_classes = lfw_people.target_names.shape[0]

X = X / 255.0

X = X.reshape((n_samples, h, w, 1))

y = to_categorical(y, num_classes=n_classes)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(h, w, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(n_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32)

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc:.2f}')

<a href="https://colab.research.google.com/drive/12td_nR6EBBn89fM6rZRQvo7T41Qshoty?usp=sharing" target="_blank">
        Open Google Colab</a>
</pre>


<h1>Language model</h1>
<pre>
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
text = """
Jack and Jill went up the hill
To fetch a pail of water
Jack fell down and broke his crown
And Jill came tumbling after

Up Jack got and home did trot
As fast as he could caper
And went to bed to mend his head
With vinegar and brown paper
"""


tokenizer = Tokenizer()
tokenizer.fit_on_texts(text.split('\n'))
total_words = len(tokenizer.word_index) + 1

input_sequences = []
for line in text.split('\n'):
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i + 1]
        input_sequences.append(n_gram_sequence)


max_sequence_length = max(len(x) for x in input_sequences)
input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')

X, y = input_sequences[:, :-1], input_sequences[:, -1]
y = tf.keras.utils.to_categorical(y, num_classes=total_words)


model = Sequential()
model.add(Embedding(total_words, 10, input_length=max_sequence_length - 1))
model.add(LSTM(150, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(100))  # Reduced LSTM units in the second layer for better performance
model.add(Dropout(0.2))
model.add(Dense(total_words, activation='softmax'))

# Compiling the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Model Checkpointing and Early Stopping
checkpoint = ModelCheckpoint("best_model.keras", monitor='loss', verbose=1, save_best_only=True, mode='min')
early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1)

# Training the model
model.fit(X, y, epochs=100, verbose=1, callbacks=[checkpoint, early_stopping])

# Function to generate text
def generate_text(seed_text, next_words, model, max_sequence_length):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')
        predicted = model.predict(token_list, verbose=0)
        predicted_word_index = np.argmax(predicted, axis=-1)
        output_word = tokenizer.index_word[predicted_word_index[0]]  # More efficient word lookup
        seed_text += " " + output_word
    return seed_text

# Example of generating text
seed_text = "Jack and Jill"
generated_text = generate_text(seed_text, 10, model, max_sequence_length)
print(generated_text)

<a href="https://colab.research.google.com/drive/1WIapz36nQQEDqtfrynEMA2FAthmoKJ9G?usp=sharing" target="_blank">
        Open Google Colab</a>
</pre>

<h1>Sentiment analysis</h1>
<pre>
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.metrics import classification_report

# Hyperparameters
vocab_size = 10000
embedding_dim = 128
max_length = 100
lstm_units = 64
batch_size = 32
epochs = 5

# Load and preprocess the data
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

# Pad sequences to ensure uniform input size
x_train = pad_sequences(x_train, maxlen=max_length)
x_test = pad_sequences(x_test, maxlen=max_length)

# Define the model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim),
    LSTM(lstm_units, return_sequences=False),
    Dropout(0.5),  # Adding dropout to prevent overfitting
    Dense(1, activation='sigmoid')  # Binary classification (positive or negative)
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test, verbose=1)
print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

# Predict sentiment
def predict_sentiment(texts):
    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)
    tokenizer.fit_on_texts([i for i in imdb.get_word_index().keys()])  # Fit tokenizer on the dataset's vocabulary
    sequences = tokenizer.texts_to_sequences(texts)
    padded_sequences = pad_sequences(sequences, maxlen=max_length)
    predictions = model.predict(padded_sequences)
    return ['Positive' if pred > 0.5 else 'Negative' for pred in predictions]

texts = ["I love this movie!", "This film was terrible."]
predictions = predict_sentiment(texts)
print(predictions)

<a href="https://colab.research.google.com/drive/1xqQg30z7NI9Y2fwhki87jwTNfsHUOfE7?usp=sharing" target="_blank">
        Open Google Colab</a>
</pre>


    <h1>Sequence to Sequence speech tagging</h1>
    <p>This is an example of a sequence-to-sequence model using LSTM for sequence labeling. The following Python code demonstrates the process:</p>
    
    <pre>
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
import tensorflow as tf
from tensorflow.keras import layers, models

sentences = [
 ["I", "love", "coding"],
 ["Dogs", "are", "great"],
 ["Cats", "are", "cute"],
 ["Birds", "sing", "beautifully"],
 ["The", "sun", "is", "shining"]
]
tags = [
 ["PRON", "VERB", "NOUN"],
 ["NOUN", "VERB", "ADJ"],
 ["NOUN", "VERB", "ADJ"],
 ["NOUN", "VERB", "ADV"],
 ["DET", "NOUN", "VERB", "VERB"]
]

word_vocab = {word: i for i, word in enumerate(set(word for sentence in sentences for word in sentence))}
tag_vocab = {tag: i for i, tag in enumerate(set(tag for tag_list in tags for tag in tag_list))}

X = [[word_vocab[word] for word in sentence] for sentence in sentences]
y = [[tag_vocab[tag] for tag in tag_list] for tag_list in tags]

max_len = max(len(seq) for seq in X)
X = pad_sequences(X, maxlen=max_len, padding='post')
y = pad_sequences(y, maxlen=max_len, padding='post')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

embedding_dim = 64
num_words = len(word_vocab)
num_tags = len(tag_vocab)

model = Sequential([
    layers.Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len),
    layers.LSTM(64, return_sequences=True),
    layers.Dense(num_tags, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

y_train = np.expand_dims(y_train, -1)
y_test = np.expand_dims(y_test, -1)

history = model.fit(X_train, y_train, batch_size=2, epochs=20, validation_split=0.2)

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')


<a href="https://colab.research.google.com/drive/16V-V_AXWt4TiOqRzWzAveY43TSD4WBCD?usp=sharing" target="_blank">
        Open Google Colab</a>
    </pre>

<h1>Machine translation encoder and decoder</h1>
<pre>
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences


input_texts = ['I love coding', 'This is a pen', 'She sings well']
target_texts = ['Ich liebe das Coden', 'Das ist ein Stift', 'Sie singt gut']


input_words = set()
target_words = set()
for input_text, target_text in zip(input_texts, target_texts):
    input_words.update(input_text.split())
    target_words.update(target_text.split())


target_words.add('<sos>')
target_words.add('<eos>')

input_word2idx = {word: idx for idx, word in enumerate(input_words)}
input_idx2word = {idx: word for idx, word in enumerate(input_words)}
target_word2idx = {word: idx for idx, word in enumerate(target_words)}
target_idx2word = {idx: word for idx, word in enumerate(target_words)}


max_encoder_seq_length = max(len(text.split()) for text in input_texts)
max_decoder_seq_length = max(len(text.split()) for text in target_texts)


encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype='int32')
for i, input_text in enumerate(input_texts):
    for t, word in enumerate(input_text.split()):
        encoder_input_data[i, t] = input_word2idx[word]


decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype='int32')
decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, len(target_words)), dtype='float32')
for i, target_text in enumerate(target_texts):
    for t, word in enumerate(target_text.split()):
        decoder_input_data[i, t] = target_word2idx[word]
        if t > 0:
            decoder_target_data[i, t - 1, target_word2idx[word]] = 1.0


encoder_inputs = Input(shape=(None,))
encoder_embedding = Embedding(len(input_words), 256)(encoder_inputs)
encoder_lstm = LSTM(256, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]


decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(len(target_words), 256)(decoder_inputs)
decoder_lstm = LSTM(256, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(len(target_words), activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)


model = Model([encoder_inputs, decoder_inputs], decoder_outputs)


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=50, validation_split=0.2)


encoder_model = Model(encoder_inputs, encoder_states)


decoder_state_input_h = Input(shape=(256,))
decoder_state_input_c = Input(shape=(256,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)


def translate(input_sequence):
    states_value = encoder_model.predict(input_sequence)
    target_sequence = np.zeros((1, 1), dtype='int32')
    target_sequence[0, 0] = target_word2idx['<sos>']

    stop_condition = False
    translation = []

    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_sequence] + states_value)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = target_idx2word[sampled_token_index]
        translation.append(sampled_word)

        if sampled_word == '<eos>' or len(translation) > max_decoder_seq_length:
            stop_condition = True

        target_sequence = np.zeros((1, 1), dtype='int32')
        target_sequence[0, 0] = sampled_token_index
        states_value = [h, c]

    return ' '.join(translation)


for input_text in input_texts:
    input_seq = pad_sequences([[input_word2idx[word] for word in input_text.split()]], maxlen=max_encoder_seq_length)
    translated_text = translate(input_seq)
    print('Input:', input_text)
    print('Translated Text:', translated_text)
    print()

<a href="https://colab.research.google.com/drive/1x7HLKmlQBl0hnDA0aZDDdCTvzbWMnO1X?usp=sharing" target="_blank">
        Open Google Colab</a>
</pre>
<h1>Image augmentatio using gan</h1>
<pre>
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import os

# Load and preprocess dataset (for example, using CIFAR-10 or your own dataset)
# For demonstration, we'll use the CIFAR-10 dataset.
(X_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()

# Normalize images to range [-1, 1]
X_train = X_train / 127.5 - 1.0

# Build the Generator
def build_generator(latent_dim):
    model = models.Sequential()
    model.add(layers.Dense(256, input_dim=latent_dim))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization(momentum=0.8))
    model.add(layers.Dense(512))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization(momentum=0.8))
    model.add(layers.Dense(1024))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization(momentum=0.8))
    model.add(layers.Dense(3072, activation='tanh'))  # CIFAR-10 images are 32x32x3, so 32*32*3 = 3072
    model.add(layers.Reshape((32, 32, 3)))
    return model

# Build the Discriminator
def build_discriminator(img_shape):
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=img_shape))
    model.add(layers.Dense(512))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dense(256))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

# Build the GAN
def build_gan(generator, discriminator):
    model = models.Sequential()
    model.add(generator)
    model.add(discriminator)
    return model

# Hyperparameters
latent_dim = 100
img_shape = (32, 32, 3)
epochs = 10000
batch_size = 64
half_batch = batch_size // 2

# Create the discriminator and the generator
discriminator = build_discriminator(img_shape)
generator = build_generator(latent_dim)

# Compile the discriminator
discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Make the discriminator non-trainable for the GAN model
discriminator.trainable = False

# Compile the GAN model
gan = build_gan(generator, discriminator)
gan.compile(loss='binary_crossentropy', optimizer='adam')

# Training the GAN
for epoch in range(epochs):
    # Train discriminator
    idx = np.random.randint(0, X_train.shape[0], half_batch)
    real_images = X_train[idx]
    
    # Generate fake images
    noise = np.random.normal(0, 1, (half_batch, latent_dim))
    fake_images = generator.predict(noise)
    
    # Train the discriminator (real images = 1, fake images = 0)
    d_loss_real = discriminator.train_on_batch(real_images, np.ones((half_batch, 1)))
    d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((half_batch, 1)))
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
    # Train generator (wants the discriminator to classify generated images as real)
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))
    
    # Print the progress
    if epoch % 1000 == 0:
        print(f"{epoch}/{epochs} | D Loss: {d_loss[0]} | G Loss: {g_loss}")

    # Save generated images every few epochs
    if epoch % 1000 == 0:
        noise = np.random.normal(0, 1, (16, latent_dim))
        gen_images = generator.predict(noise)
        gen_images = 0.5 * gen_images + 0.5  # Rescale to [0, 1] for visualization
        
        # Plot and save the generated images
        fig, axs = plt.subplots(4, 4, figsize=(4, 4))
        count = 0
        for i in range(4):
            for j in range(4):
                axs[i, j].imshow(gen_images[count])
                axs[i, j].axis('off')
                count += 1
        plt.show()

# Example of augmenting a single image
def augment_image(image_path):
    img = image.load_img(image_path, target_size=(32, 32))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = img_array / 127.5 - 1.0  # Normalize image
    
    noise = np.random.normal(0, 1, (1, latent_dim))
    generated_img = generator.predict(noise)
    generated_img = 0.5 * generated_img + 0.5  # Rescale to [0, 1]

    # Show the augmented image
    plt.imshow(generated_img[0])
    plt.axis('off')
    plt.show()

# Example usage:
augment_image('path_to_your_image.jpg')  # Provide the path to your image file
<a href="https://colab.research.google.com/drive/1jrlQi2BFEfZ4QehzALUC2C6n61etio9n?usp=sharing" target="_blank">
        Open Google Colab</a>
</pre>
</body>
</html>
